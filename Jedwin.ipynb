{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Description](https://www.kaggle.com/competitions/comp90051-2024s1-project-1)\n",
    "Text generation has become an increasingly popular task with the rise of natural language processing (NLP) techniques and advancements in deep learning. Given a short text prompt written by a human, text generation employs overparameterised models to generate response text: a likely sequence of text that might follow the prompt, based on enormous training datasets of text from news articles, online libraries of books, and from scraping the web. While text generation has a wide range of applications, including chatbots, language translation, and content creation, it also poses a significant challenge in ensuring content authenticity, accuracy, and authoritativeness. This is where text generation detection comes in, which is the process of identifying whether a given text is machine-generated or human-written. Developing effective text generation detection models is important because it can help prevent the spread of fake news, misinformation, and propaganda.\n",
    "\n",
    "Your task is to come up with test predictions for a machine-generated detection problem given a training set and test input instances. That is, your task is to predict whether given text input instances have been generated by a human or a machine.\n",
    "\n",
    "Full details for this task are provided in the assignment description on Canvas.\n",
    "\n",
    "\n",
    "## [Evaluation](www.kaggle.com/competitions/comp90051-2024s1-project-1/overview/evaluation)\n",
    "For all participants, the same 50% of predictions from the test set are assigned to the public leaderboard. The score you see on the public leaderboard reflects your modelâ€™s accuracy on this portion of the test set. The other 50% of the test set will only be used once, after the competition has closed, to determine your final ranking and accuracy scores. This means that you must take care not to overfit to the leaderboard.\n",
    "\n",
    "Submissions will be evaluated using the classification accuracy between the predicted class and the observed target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import json\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datasets\n",
    "with open(\"domain1_train_data.json\", \"r\") as f:\n",
    "    dataset_1 = [ json.loads(line, parse_int = str) for line in f ]\n",
    "\n",
    "with open(\"domain2_train_data.json\", \"r\") as f:\n",
    "    dataset_2 = [ json.loads(line, parse_int = str) for line in f ]\n",
    "\n",
    "with open(\"test_data.json\", \"r\") as f:\n",
    "    testset = [ json.loads(line, parse_int = str) for line in f ]\n",
    "\n",
    "n_samples_1 = len(dataset_1)\n",
    "n_samples_2 = len(dataset_2)\n",
    "n_tests = len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a vocabulary of 1-grams, 2-grams, 3-grams (1-word, 2-word, 3-word)\n",
    "vocab = {}\n",
    "for m in range(n_samples_1):\n",
    "    text = dataset_1[m]['text']\n",
    "    textlength = len(text)\n",
    "    for i in range(textlength):\n",
    "        onegram = f\"{text[i]}\"\n",
    "        vocab[onegram] = vocab.get(onegram, 0) + 1\n",
    "        if i < textlength-1: \n",
    "            twogram = f\"{text[i]} {text[i+1]}\"\n",
    "            vocab[twogram] = vocab.get(twogram, 0) + 1\n",
    "        # if i < textlength-2:\n",
    "        #     threegram = f\"{text[i]} {text[i+1]} {text[i+2]}\"\n",
    "        #     vocab[threegram] = vocab.get(threegram, 0) + 1\n",
    "        \n",
    "    \n",
    "for m in range(n_samples_2):\n",
    "    text = dataset_2[m]['text']\n",
    "    textlength = len(text)\n",
    "    for i in range(textlength):\n",
    "        onegram = f\"{text[i]}\"\n",
    "        vocab[onegram] = vocab.get(onegram, 0) + 1\n",
    "        if i < textlength-1: \n",
    "            twogram = f\"{text[i]} {text[i+1]}\"\n",
    "            vocab[twogram] = vocab.get(twogram, 0) + 1\n",
    "        # if i < textlength-2:\n",
    "        #     threegram = f\"{text[i]} {text[i+1]} {text[i+2]}\"\n",
    "        #     vocab[threegram] = vocab.get(threegram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  1040230\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of vocabulary: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To reduce number of features, just choose ALL 1-grams (like bag of words)\n",
    "#Then include the top most occuring (2,3)-grams\n",
    "# final_vocab = {str(i): i for i in range(83583)} #1-grams\n",
    "# s = 0\n",
    "final_vocab = {}\n",
    "t = 0\n",
    "number_1gram = 0\n",
    "number_2gram = 0\n",
    "\n",
    "#Remove unique words and n-grams in the final vocabulary, as they will act as noise in classifying.\n",
    "for word, count in vocab.items():\n",
    "    if (count > 1) and (\" \" not in word): #1-grams don't have space\n",
    "        final_vocab[ word ] = t\n",
    "        number_1gram += 1\n",
    "        t += 1\n",
    "        \n",
    "    elif (count > 1) and (\" \" in word):\n",
    "        final_vocab[ word ] = t\n",
    "        number_2gram += 1\n",
    "        t += 1\n",
    "\n",
    "# sorted_vocabs = sorted( vocab.items(), key=lambda x:x[1] )\n",
    "# number_ngrams_included = 10000\n",
    "# while s < number_ngrams_included:\n",
    "#     word, count = sorted_vocabs.pop()\n",
    "#     if word not in final_vocab:\n",
    "#         final_vocab[str(word)] = 83583+s\n",
    "#         s += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_vocab.json\", \"w\") as f:\n",
    "    json.dump(final_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def feature_select( texts: list[str], *, vocabulary: dict = None, method=\"countvectorize\", sparse=False, **kwargs ):\n",
    "    \"\"\"From a list of texts, output a dataframe of features with shape (n_samples, n_features).\n",
    "\n",
    "     Args:\n",
    "         texts (list[str]): list of strings, each item corresponding to a text.\n",
    "         vocabulary (dict, optional): _description_. Defaults to None.\n",
    "         method (str, optional): Method to select features. Defaults to \"count-vectorizer\".\n",
    "         **kwargs: kwarg arguments to pass to Vectorizer classes of sklearn.\n",
    "    Raises:\n",
    "        ValueError: If passing an non-specified method of text feature extraction\n",
    "\n",
    "     Returns:\n",
    "         pd.DataFrame: dataframe of shape (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    #We want single digits to tokenized. This regex considers everything as a token except whitespace.\n",
    "    kwargs['token_pattern'] = r'\\S+' \n",
    "    if method == \"countvectorize\":\n",
    "        vectorizer = CountVectorizer(vocabulary = vocabulary, **kwargs) if vocabulary else CountVectorizer(**kwargs)\n",
    "    elif method == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(vocabulary = vocabulary, **kwargs) if vocabulary else TfidfVectorizer(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"{method} is not a supported method.\")\n",
    "    if not sparse:\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        df = pd.DataFrame.sparse.from_spmatrix(data=X, columns = feature_names)\n",
    "        return df, vectorizer\n",
    "    else:\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "        return X, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn lists of words from dataset into sentences:\n",
    "datatexts = []\n",
    "for dataset in [dataset_1, dataset_2]:\n",
    "    for instance in dataset:\n",
    "        datatexts += [ \" \".join(instance[\"text\"]) ]\n",
    "testtexts = []\n",
    "for instance in testset:\n",
    "    testtexts += [ \" \".join(instance[\"text\"]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 335618)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Selection:\n",
    "with open(\"final_vocab.json\", \"r\") as f:\n",
    "    final_vocab = json.load(f)\n",
    "    \n",
    "#Need lots of memory, use sparse method matrix instead then use incremental PCA\n",
    "X, vectorizer = feature_select(texts = datatexts, \n",
    "                    vocabulary=final_vocab, \n",
    "                    method='tfidf',\n",
    "                    ngram_range=(1,2)\n",
    "                    )\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels\n",
    "y = [ [dataset_1[i]['label']] for i in range(n_samples_1) ] \n",
    "y += [ [dataset_2[i]['label']] for i in range(n_samples_2) ]\n",
    "y = pd.DataFrame( y, columns=[\"label\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA, PCA, SparsePCA\n",
    "from scipy import sparse\n",
    "\n",
    "#sparse matrix feature selection\n",
    "#Feature Selection:\n",
    "X_sparse, vectorizer = feature_select(texts = datatexts, \n",
    "                    vocabulary=final_vocab, \n",
    "                    method='tfidf',\n",
    "                    ngram_range=(1,2),\n",
    "                    sparse=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 335618)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 1040230)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Repeat for full vocab\n",
    "# X_sparse_2, vectorizer_2 = feature_select(texts = datatexts, \n",
    "#                     vocabulary=vocab.keys(), \n",
    "#                     method='tfidf',\n",
    "#                     ngram_range=(1,2),\n",
    "#                     sparse=True\n",
    "#                     )\n",
    "# X_sparse_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture mutual_info\n",
    "%%time\n",
    "mutual_info_classif( X, y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture select_kbest\n",
    "%%time\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif\n",
    "selector = SelectKBest( mutual_info_classif, k = 20000 )\n",
    "X_reduced = selector.fit_transform( X )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture f_classif\n",
    "%%time\n",
    "f_classif( X, y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture PCA\n",
    "%%time\n",
    "#Use incremental PCA to reduce to 4500 new eigen-features (lin. combs of original features) with highest variance \n",
    "transformer = IncrementalPCA(n_components=9000, batch_size=9000)\n",
    "X_pca = transformer.fit_transform(X_sparse)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture output_2\n",
    "# transformer_2 = IncrementalPCA(n_components=4500, batch_size=4500)\n",
    "# X_2 = transformer_2.fit_transform(X_sparse_2)\n",
    "# X_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 4500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    train_size=0.8, \n",
    "                                                    random_state=2024, \n",
    "                                                    shuffle=True, \n",
    "                                                    stratify=None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = MinMaxScaler().fit_transform(X_train.to_numpy(dtype=np.float64))\n",
    "X_test = MinMaxScaler().fit_transform(X_test.to_numpy(dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jvillanueva/anaconda3/envs/statML/lib/python3.11/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7766666666666666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jvillanueva/anaconda3/envs/statML/lib/python3.11/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but MultinomialNB was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "testtexts = [ testset[i]['text'] for i in range(n_tests) ]\n",
    "testtexts = [ \" \".join(testtexts[i]) for i in range(n_tests) ]\n",
    "X_test_featureset = feature_select(texts = testtexts, \n",
    "                                         vocabulary=final_vocab,\n",
    "                                         method=\"tfidf\",\n",
    "                                         ngram_range=(1,2)\n",
    "                                        )\n",
    "predictions = classifier.predict( X_test_featureset )\n",
    "predictions = pd.DataFrame( predictions, index=range(n_tests), columns=[ \"class\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\"sample.csv\", sep=\",\", header=True, index_label=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0        2072\n",
       "1        1928\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.3k/26.3k [00:01<00:00, 16.3kB/s]\n",
      "Successfully submitted to COMP90051 2024S1 Project 1"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c comp90051-2024s1-project-1 -f sample.csv -m \"Trial\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
