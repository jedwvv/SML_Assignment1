{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a096f214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 1 label frequencies:\n",
      " 1    2500\n",
      "0    2500\n",
      "Name: label, dtype: int64\n",
      "Domain 2 label frequencies:\n",
      " 0    11500\n",
      "1     1500\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "domain1_df = pd.read_json('domain1_train_data.json', lines=True)\n",
    "domain2_df = pd.read_json('domain2_train_data.json', lines=True)\n",
    "\n",
    "# Calculate label frequencies for domain 1\n",
    "label_counts_d1 = domain1_df['label'].value_counts()\n",
    "\n",
    "# Calculate label frequencies for domain 2\n",
    "label_counts_d2 = domain2_df['label'].value_counts()\n",
    "\n",
    "print(\"Domain 1 label frequencies:\\n\", label_counts_d1)\n",
    "print(\"Domain 2 label frequencies:\\n\", label_counts_d2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7adf1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import GradientBoostingClassifier, IsolationForest, RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "981940e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def load_data(file_path):\n",
    "    return pd.read_json(file_path, lines=True)\n",
    "\n",
    "df_domain1 = load_data('domain1_train_data.json')\n",
    "df_domain2 = load_data('domain2_train_data.json')\n",
    "df_test = load_data('test_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e364fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine texts from both domains and test data for vectorization\n",
    "all_texts = pd.concat([df_domain1['text'], df_domain2['text'], df_test['text']]).apply(lambda x: ' '.join(map(str, x)))\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_all = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "# Split the vectorized data back into domain1, domain2, and test sets\n",
    "X_domain1 = X_all[:len(df_domain1)]\n",
    "X_domain2 = X_all[len(df_domain1):-len(df_test)]\n",
    "X_test = X_all[-len(df_test):]\n",
    "\n",
    "# Save vectorizer for later use\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc5bbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Classifier Accuracy: 0.9975\n"
     ]
    }
   ],
   "source": [
    "# Create domain labels (0 for domain1, 1 for domain2)\n",
    "y_domains = [0]*len(df_domain1) + [1]*len(df_domain2)\n",
    "X_domains = vectorizer.transform(pd.concat([df_domain1['text'], df_domain2['text']]).apply(lambda x: ' '.join(map(str, x))))\n",
    "\n",
    "# Train-test split\n",
    "X_train_domain, X_val_domain, y_train_domain, y_val_domain = train_test_split(X_domains, y_domains, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train domain classifier\n",
    "clf1 = LogisticRegression(random_state=42)\n",
    "clf2 = RandomForestClassifier(random_state=42)\n",
    "clf3 = SVC(probability=True, random_state=42)\n",
    "\n",
    "domain_classifier = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "domain_classifier.fit(X_train_domain, y_train_domain)\n",
    "\n",
    "# Save domain classifier\n",
    "joblib.dump(domain_classifier, 'domain_classifier.pkl')\n",
    "\n",
    "y_pred_domain = domain_classifier.predict(X_val_domain)\n",
    "print(\"Domain Classifier Accuracy:\", accuracy_score(y_val_domain, y_pred_domain))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03d98ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing text data for domain 1\n",
    "df_domain1['text_str'] = df_domain1['text'].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "tfidf_vectorizer_d1 = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_domain1 = tfidf_vectorizer_d1.fit_transform(df_domain1['text_str'])\n",
    "\n",
    "# Splitting the data into training and validation sets for domain 1\n",
    "X_train_d1, X_val_d1, y_train_d1, y_val_d1 = train_test_split(X_domain1, df_domain1['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d0c3498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer_d1.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the stacking classifier\n",
    "base_learners = [\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)), \n",
    "    ('svc', SVC(probability=True, random_state=42)), \n",
    "    ('lr', LogisticRegression(random_state=42)), \n",
    "    ('knn', KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "# Meta-learner\n",
    "meta_learner = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Stacking classifier\n",
    "stacking_cls_d1 = StackingClassifier(estimators=base_learners, final_estimator=meta_learner, cv=5)\n",
    "\n",
    "# Train the stacking classifier\n",
    "stacking_cls_d1.fit(X_train_d1, y_train_d1)\n",
    "\n",
    "# Prediction on the test set\n",
    "y_pred_d1 = stacking_cls_d1.predict(X_val_d1)\n",
    "\n",
    "# Save classifiers\n",
    "joblib.dump(stacking_cls_d1, 'ai_human_classifier_d1.pkl')\n",
    "joblib.dump(tfidf_vectorizer_d1,'tfidf_vectorizer_d1.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9978204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Domain 1): 0.828\n",
      "F1-Score (Domain 1): 0.8323586744639376\n",
      "ROC-AUC (Domain 1): 0.8280000000000001\n"
     ]
    }
   ],
   "source": [
    "# Evaluation (example for domain 1)\n",
    "print(\"Accuracy (Domain 1):\", accuracy_score(y_val_d1, y_pred_d1))\n",
    "print(\"F1-Score (Domain 1):\", f1_score(y_val_d1, y_pred_d1))\n",
    "print(\"ROC-AUC (Domain 1):\", roc_auc_score(y_val_d1, y_pred_d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2488696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing text data for domain 2\n",
    "df_domain2['text_str'] = df_domain2['text'].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "tfidf_vectorizer_d2 = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_domain2 = tfidf_vectorizer_d2.fit_transform(df_domain2['text_str'])\n",
    "y_domain2 = df_domain2['label']\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "X_train_d2, X_val_d2, y_train_d2, y_val_d2 = train_test_split(X_domain2, y_domain2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87bb43ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer_d2.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_d2, y_train_d2)\n",
    "\n",
    "# Initialize a classifier, for example, RandomForest\n",
    "stacking_cls_d2 = StackingClassifier(estimators=base_learners, final_estimator=meta_learner, cv=5)\n",
    "\n",
    "# Train the classifier on the oversampled training data\n",
    "stacking_cls_d2.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_d2 = stacking_cls_d2.predict(X_val_d2)\n",
    "\n",
    "# Save classifiers\n",
    "joblib.dump(stacking_cls_d2, 'ai_human_classifier_d2.pkl')\n",
    "joblib.dump(tfidf_vectorizer_d2,'tfidf_vectorizer_d2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a329db54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9084615384615384\n",
      "Accuracy (Domain 2): 0.9084615384615384\n",
      "F1-Score (Domain 2): 0.4079601990049751\n",
      "ROC-AUC (Domain 2): 0.6298668734433801\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_val_d2, y_pred_d2))\n",
    "print(\"Accuracy (Domain 2):\", accuracy_score(y_val_d2, y_pred_d2))\n",
    "print(\"F1-Score (Domain 2):\", f1_score(y_val_d2, y_pred_d2))\n",
    "print(\"ROC-AUC (Domain 2):\", roc_auc_score(y_val_d2, y_pred_d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ee98cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved models\n",
    "domain_classifier = joblib.load('domain_classifier.pkl')\n",
    "ai_human_classifier_d1 = joblib.load('ai_human_classifier_d1.pkl')\n",
    "ai_human_classifier_d2 = joblib.load('ai_human_classifier_d2.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd0c8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctly load pre-fitted vectorizers\n",
    "tfidf_vectorizer_domain = joblib.load('vectorizer.pkl')  # Adjust file path as necessary\n",
    "\n",
    "# Assuming df_test is your test DataFrame and it includes a column 'text' containing tokenized texts\n",
    "df_test['text_str'] = df_test['text'].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "\n",
    "# Correctly use the pre-fitted vectorizers to transform the test data\n",
    "X_test_domain = tfidf_vectorizer_domain.transform(df_test['text_str'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4b83122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/tvj0_bqn32v820xmvm68s3880000gp/T/ipykernel_59074/3257377892.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_d1['text_str'] = df_test_d1['text'].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
      "/var/folders/wj/tvj0_bqn32v820xmvm68s3880000gp/T/ipykernel_59074/3257377892.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_d2['text_str'] = df_test_d2['text'].apply(lambda tokens: ' '.join(map(str, tokens)))\n"
     ]
    }
   ],
   "source": [
    "# Predict domain for test data\n",
    "test_domain_preds = domain_classifier.predict(X_test_domain)\n",
    "\n",
    "# Splitting the test data based on domain predictions\n",
    "df_test['predicted_domain'] = test_domain_preds  # Assign domain predictions to a new column in the test DataFrame\n",
    "df_test_d1 = df_test[df_test['predicted_domain'] == 0]  # Filter rows predicted as Domain 1\n",
    "df_test_d2 = df_test[df_test['predicted_domain'] == 1]  # Filter rows predicted as Domain 2\n",
    "\n",
    "# Assuming df_test is your test DataFrame and it includes a column 'text' containing tokenized texts\n",
    "df_test_d1['text_str'] = df_test_d1['text'].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "df_test_d2['text_str'] = df_test_d2['text'].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "\n",
    "# Assuming df_test already has a 'text_str' column from previous steps\n",
    "# Transform the test data for AI vs Human classification\n",
    "X_test_d1 = tfidf_vectorizer_domain.transform(df_test_d1['text_str'])\n",
    "X_test_d2 = tfidf_vectorizer_domain.transform(df_test_d2['text_str'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4791825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/tvj0_bqn32v820xmvm68s3880000gp/T/ipykernel_59074/1585613002.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_d1['Predicted_Label'] = test_ai_human_preds_d1\n",
      "/var/folders/wj/tvj0_bqn32v820xmvm68s3880000gp/T/ipykernel_59074/1585613002.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_d2['Predicted_Label'] = test_ai_human_preds_d2\n"
     ]
    }
   ],
   "source": [
    "# Classify between AI and Human \n",
    "test_ai_human_preds_d1 = ai_human_classifier_d1.predict(X_test_d1)\n",
    "test_ai_human_preds_d2 = ai_human_classifier_d2.predict(X_test_d2)\n",
    "\n",
    "# Add prediced labels to CSV\n",
    "df_test_d1['Predicted_Label'] = test_ai_human_preds_d1\n",
    "df_test_d2['Predicted_Label'] = test_ai_human_preds_d2\n",
    "\n",
    "# Combine both predictions\n",
    "df_final_predictions = pd.concat([df_test_d1, df_test_d2]).sort_index()\n",
    "\n",
    "# Select only 'id', 'predicted_domain', and 'Predicted_Label' columns\n",
    "df_final_predictions = df_final_predictions[['id', 'predicted_domain', 'Predicted_Label']]\n",
    "\n",
    "# Save to CSV\n",
    "df_final_predictions.to_csv('final_predictions_with_domain.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06ab443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    2007\n",
      "0    1993\n",
      "Name: predicted_domain, dtype: int64\n",
      "0    2512\n",
      "1    1488\n",
      "Name: Predicted_Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'final_predictions.csv' is your saved CSV file\n",
    "csv_path = 'final_predictions_with_domain.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Assuming 'Predicted_Label' is the column with your labels\n",
    "domain_counts = df['predicted_domain'].value_counts()\n",
    "label_counts = df['Predicted_Label'].value_counts()\n",
    "\n",
    "print(domain_counts)\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65c21965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a90c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['domain_classifier.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets\n",
    "def load_data(file_path):\n",
    "    return pd.read_json(file_path, lines=True)\n",
    "\n",
    "df_domain1 = load_data('domain1_train_data.json')\n",
    "df_domain2 = load_data('domain2_train_data.json')\n",
    "df_test = load_data('test_data.json')\n",
    "\n",
    "# Preprocessing function to convert lists of tokens into a single string per text\n",
    "def preprocess_texts(df):\n",
    "    df['text_str'] = df['text'].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "    return df\n",
    "\n",
    "df_domain1 = preprocess_texts(df_domain1)\n",
    "df_domain2 = preprocess_texts(df_domain2)\n",
    "df_test = preprocess_texts(df_test)\n",
    "\n",
    "# Combine texts for domain classification\n",
    "all_texts = pd.concat([df_domain1['text_str'], df_domain2['text_str']])\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_all = vectorizer.fit_transform(all_texts)\n",
    "y_domains = np.array([0]*len(df_domain1) + [1]*len(df_domain2))\n",
    "\n",
    "# Train-test split for domain classification\n",
    "X_train_domain, X_val_domain, y_train_domain, y_val_domain = train_test_split(X_all, y_domains, test_size=0.2, random_state=42)\n",
    "\n",
    "# Domain classification model\n",
    "domain_classifier = LogisticRegression(random_state=42)\n",
    "domain_classifier.fit(X_train_domain, y_train_domain)\n",
    "joblib.dump(domain_classifier, 'domain_classifier.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87152aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and sequence padding for LSTM\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df_domain1['text_str'].tolist() + df_domain2['text_str'].tolist())\n",
    "sequences_domain1 = tokenizer.texts_to_sequences(df_domain1['text_str'].tolist())\n",
    "sequences_domain2 = tokenizer.texts_to_sequences(df_domain2['text_str'].tolist())\n",
    "max_len = max(max([len(seq) for seq in sequences_domain1]), max([len(seq) for seq in sequences_domain2]))\n",
    "X_seq_domain1 = pad_sequences(sequences_domain1, maxlen=max_len)\n",
    "X_seq_domain2 = pad_sequences(sequences_domain2, maxlen=max_len)\n",
    "y_seq_domain1 = df_domain1['label'].values\n",
    "y_seq_domain2 = df_domain2['label'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bfe2624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Classifier Accuracy: 0.9975\n"
     ]
    }
   ],
   "source": [
    "# Train-test split for domain classification\n",
    "X_train_domain, X_val_domain, y_train_domain, y_val_domain = train_test_split(X_all, y_domains, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the classifiers for the VotingClassifier\n",
    "clf1 = LogisticRegression(random_state=42)\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf3 = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Train domain classifier using VotingClassifier\n",
    "domain_classifier = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)], voting='soft')\n",
    "domain_classifier.fit(X_train_domain, y_train_domain)\n",
    "\n",
    "# Save domain classifier\n",
    "joblib.dump(domain_classifier, 'domain_classifier.pkl')\n",
    "\n",
    "# Evaluate domain classifier\n",
    "y_pred_domain = domain_classifier.predict(X_val_domain)\n",
    "print(f\"Domain Classifier Accuracy: {accuracy_score(y_val_domain, y_pred_domain)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea07c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM Model\n",
    "def define_lstm_model(vocab_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=100))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=L1L2(l1=1e-5, l2=1e-4)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65fc0603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 2s/step - accuracy: 0.6087 - loss: 0.6953 - val_accuracy: 0.0540 - val_loss: 0.9801\n",
      "Epoch 2/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.7013 - loss: 0.5877 - val_accuracy: 0.4500 - val_loss: 0.8200\n",
      "Epoch 3/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.7763 - loss: 0.5016 - val_accuracy: 0.5280 - val_loss: 0.8216\n",
      "Epoch 4/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.8317 - loss: 0.4052 - val_accuracy: 0.5520 - val_loss: 0.8837\n",
      "Epoch 5/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1s/step - accuracy: 0.8469 - loss: 0.3783 - val_accuracy: 0.5090 - val_loss: 1.0242\n",
      "Epoch 6/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.8706 - loss: 0.3346 - val_accuracy: 0.5550 - val_loss: 1.0155\n",
      "Epoch 7/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.8901 - loss: 0.2917 - val_accuracy: 0.5670 - val_loss: 1.1111\n",
      "Epoch 8/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.8949 - loss: 0.2777 - val_accuracy: 0.6040 - val_loss: 0.9995\n",
      "Epoch 9/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1s/step - accuracy: 0.9239 - loss: 0.2352 - val_accuracy: 0.5890 - val_loss: 1.1331\n",
      "Epoch 10/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1s/step - accuracy: 0.9238 - loss: 0.2188 - val_accuracy: 0.4700 - val_loss: 1.3896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Train LSTM models for each domain\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model_domain1 = define_lstm_model(vocab_size)\n",
    "model_domain1.fit(X_seq_domain1, y_seq_domain1, epochs=10, batch_size=64, validation_split=0.2)\n",
    "model_domain1.save('lstm_domain1.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db090fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 1s/step - accuracy: 0.7237 - loss: 0.5527 - val_accuracy: 0.9017 - val_loss: 0.2717\n",
      "Epoch 2/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 1s/step - accuracy: 0.9160 - loss: 0.2492 - val_accuracy: 0.9145 - val_loss: 0.2449\n",
      "Epoch 3/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 1s/step - accuracy: 0.9467 - loss: 0.1713 - val_accuracy: 0.9445 - val_loss: 0.1745\n",
      "Epoch 4/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 2s/step - accuracy: 0.9611 - loss: 0.1306 - val_accuracy: 0.9368 - val_loss: 0.2115\n",
      "Epoch 5/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 2s/step - accuracy: 0.9764 - loss: 0.0934 - val_accuracy: 0.9472 - val_loss: 0.1815\n",
      "Epoch 6/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 2s/step - accuracy: 0.9701 - loss: 0.1069 - val_accuracy: 0.9506 - val_loss: 0.1717\n",
      "Epoch 7/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 2s/step - accuracy: 0.9861 - loss: 0.0645 - val_accuracy: 0.9287 - val_loss: 0.2662\n",
      "Epoch 8/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 2s/step - accuracy: 0.9786 - loss: 0.0821 - val_accuracy: 0.9477 - val_loss: 0.2291\n",
      "Epoch 9/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 2s/step - accuracy: 0.9868 - loss: 0.0562 - val_accuracy: 0.9521 - val_loss: 0.1952\n",
      "Epoch 10/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 2s/step - accuracy: 0.9890 - loss: 0.0487 - val_accuracy: 0.9653 - val_loss: 0.1619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Tokenization and sequence padding\n",
    "tokenizer.fit_on_texts(df_domain2['text_str'].tolist())\n",
    "sequences_domain2 = tokenizer.texts_to_sequences(df_domain2['text_str'].tolist())\n",
    "X_seq_domain2 = pad_sequences(sequences_domain2, maxlen=max_len)\n",
    "y_seq_domain2 = df_domain2['label'].values\n",
    "\n",
    "# Find the number of samples in the minority and majority classes\n",
    "counter = Counter(y_seq_domain2)\n",
    "minority_class = min(counter, key=counter.get)\n",
    "majority_class = max(counter, key=counter.get)\n",
    "\n",
    "# Calculate replication factor for minority class to match majority class count\n",
    "replication_factor = counter[majority_class] // counter[minority_class]\n",
    "\n",
    "# Oversample the minority class\n",
    "minority_indices = np.where(y_seq_domain2 == minority_class)[0]\n",
    "oversampled_minority_seq = np.repeat(X_seq_domain2[minority_indices], replication_factor, axis=0)\n",
    "oversampled_minority_labels = np.repeat(y_seq_domain2[minority_indices], replication_factor)\n",
    "\n",
    "# Combine the oversampled minority class with the original dataset\n",
    "X_seq_domain2_oversampled = np.vstack((X_seq_domain2, oversampled_minority_seq))\n",
    "y_seq_domain2_oversampled = np.hstack((y_seq_domain2, oversampled_minority_labels))\n",
    "\n",
    "# Train-test split\n",
    "X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split(X_seq_domain2_oversampled, y_seq_domain2_oversampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the LSTM model on the balanced dataset\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max_len  # Make sure max_len is correctly defined to cover all sequences\n",
    "model_domain2 = define_lstm_model(vocab_size)\n",
    "model_domain2.fit(X_train_seq, y_train_seq, epochs=10, batch_size=64, validation_data=(X_val_seq, y_val_seq))\n",
    "\n",
    "# Save your trained model\n",
    "model_domain2.save('lstm_domain2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e000369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "ai_human_classifier_d1 = load_model('lstm_domain1.h5')\n",
    "ai_human_classifier_d2 = load_model('lstm_domain2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c33ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df_test['text_str'] = df_test['text'].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "\n",
    "# Transform test data for domain classification\n",
    "X_test_domain = vectorizer.transform(df_test['text_str'])\n",
    "\n",
    "# Predict domain for test data\n",
    "test_domain_preds = domain_classifier.predict(X_test_domain)\n",
    "\n",
    "# Splitting the test data based on domain predictions\n",
    "df_test['predicted_domain'] = test_domain_preds\n",
    "df_test_d1 = df_test[df_test['predicted_domain'] == 0]\n",
    "df_test_d2 = df_test[df_test['predicted_domain'] == 1]\n",
    "\n",
    "# Transform text for AI vs. Human classification using LSTMs\n",
    "sequences_test_d1 = tokenizer.texts_to_sequences(df_test_d1['text_str'].tolist())\n",
    "sequences_test_d2 = tokenizer.texts_to_sequences(df_test_d2['text_str'].tolist())\n",
    "\n",
    "max_sequence_length_d1 = ai_human_classifier_d1.input_shape[1]\n",
    "max_sequence_length_d2 = ai_human_classifier_d2.input_shape[1]\n",
    "\n",
    "X_test_d1_seq = pad_sequences(sequences_test_d1, maxlen=max_sequence_length_d1)\n",
    "X_test_d2_seq = pad_sequences(sequences_test_d2, maxlen=max_sequence_length_d2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8668f587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 281ms/step\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 286ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/tvj0_bqn32v820xmvm68s3880000gp/T/ipykernel_79726/4017563957.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_d1['Predicted_Label'] = test_ai_human_preds_d1\n",
      "/var/folders/wj/tvj0_bqn32v820xmvm68s3880000gp/T/ipykernel_79726/4017563957.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_d2['Predicted_Label'] = test_ai_human_preds_d2\n"
     ]
    }
   ],
   "source": [
    "# Predict AI vs Human for split test data\n",
    "test_ai_human_preds_d1 = ai_human_classifier_d1.predict(X_test_d1_seq)\n",
    "test_ai_human_preds_d2 = ai_human_classifier_d2.predict(X_test_d2_seq)\n",
    "\n",
    "# Convert predictions to binary labels\n",
    "test_ai_human_preds_d1 = (test_ai_human_preds_d1.flatten() > 0.5).astype(int)\n",
    "test_ai_human_preds_d2 = (test_ai_human_preds_d2.flatten() > 0.5).astype(int)\n",
    "\n",
    "# Combine predictions and add to CSV\n",
    "df_test_d1['Predicted_Label'] = test_ai_human_preds_d1\n",
    "df_test_d2['Predicted_Label'] = test_ai_human_preds_d2\n",
    "df_final_predictions = pd.concat([df_test_d1[['id', 'predicted_domain', 'Predicted_Label']], df_test_d2[['id', 'predicted_domain', 'Predicted_Label']]]).sort_index()\n",
    "\n",
    "# Save to CSV\n",
    "df_final_predictions.to_csv('final_predictions_with_domain.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c23e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    2007\n",
      "0    1993\n",
      "Name: predicted_domain, dtype: int64\n",
      "0    2377\n",
      "1    1623\n",
      "Name: Predicted_Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'final_predictions.csv' is your saved CSV file\n",
    "csv_path = 'final_predictions_with_domain.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Assuming 'Predicted_Label' is the column with your labels\n",
    "domain_counts = df['predicted_domain'].value_counts()\n",
    "label_counts = df['Predicted_Label'].value_counts()\n",
    "\n",
    "print(domain_counts)\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a3b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
